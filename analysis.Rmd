---
title: "Practical Machine Learning Course Project"
author: "Christie Haskell"
date: "Monday, November 02, 2015"
output: html_document
---

The purpose of this project was to predict the quality of barbell lifts from physics data collected from accelerometers worn on the body of participants (belt, forearm and arm) and the dumbbel they were lifting. More information about the method and dataset is available [here]( http://groupware.les.inf.puc-rio.br/har).

**Reading in the data**
```{r,echo=TRUE}
### Change this to the directory of the data on your machine ###
setwd("C:/Users/Christie/Documents/Coursera/PracMachLearn/data")

#Read the training and test data
library(data.table)
train<-fread("pml-training.csv")
test<-fread("pml-testing.csv")
```

**Exploring the data**

What does the data look like?

```{r,echo=TRUE}
names(train)

#Remove V1, it's just a row number
train<-train[,V1:=NULL]
test<-test[,V1:=NULL]
```

It looks like there are two types of data here: the measurement at each time point during the movement, and the summary statistics for those measurements for each repetition. Does the `new_window` variable correspond with columns containing summary statistics?

```{r,echo=TRUE}
#Trying a few of the summary statistics
which(train$kurtosis_roll_belt!="") == which(train$new_window=="yes")
which(train$skewness_roll_belt!="") == which(train$new_window=="yes")
which(train$min_roll_belt!="NA") == which(train$new_window=="yes")

test[which(test$new_window=="yes"),]
```

Yes, that does appear to be the case. The summary statistics are: kurtosis, skewness, max, min, amplitude, total, var, avg and stddev. There are no values for the summary statistics in the test data set; therefore, the summary statistics in both datasets were discarded.

```{r,echo=TRUE}
library(dplyr)
library(tidyr)

train2<-tbl_df(subset(train,train$new_window=="no"))
test2<-tbl_df(subset(test,test$new_window=="no"))

remSumm <- function(df) {
  df<-df %>%
  select(-matches("user_name"),
         -matches("new_window"),
         -matches("num_window"),
         -matches("raw_timestamp_part_1"),
         -matches("raw_timestamp_part_2"),
         -matches("cvtd_timestamp"),
         -matches("problem_id"),
         -starts_with("kurtosis"),
         -starts_with("skewness"),
         -starts_with("max"),
         -starts_with("min"),
         -starts_with("amplitude"),
         -starts_with("total"),
         -starts_with("var"),
         -starts_with("avg"),
         -starts_with("stddev"))
}

train2<-remSumm(train2)
test2<-remSumm(test2)
```

**Model: Random Forest**

Random forest models are excellent classifiers. They improve upon decision trees by correcting for the overfitting of the training set that is common with decision trees. Further, they use permutation to evaluate feature importance and use this result to automatically select the important features. It is also handles correlated features and outliers well. It is for these reasons that I have opted to use a random forest model. Cross-validation is 10-fold.

```{r,echo=TRUE}
library(caret)
#Create training and testing set partitions
partIndex<-createDataPartition(train2$classe, times = 1,p = 0.7)
train.tr<-train2[partIndex$Resample1,]
train.test<-train2[-partIndex$Resample1,]

mrf <- train(as.factor(classe) ~ ., data=train.tr, method="rf", trControl=trainControl(method="cv", 10), ntree=300)
mrf
```

What features are important?
```{r,echo=TRUE}
library(ggplot2)
varImport<-varImp(mrf)
dplot<-data.frame("x"=rownames(varImport$importance),"y"=varImport$importance$Overall)
dplot<-transform(dplot,x = reorder(x, y))
plot<-ggplot(dplot[1:20,],aes(x=x,y=y,fill=y))+geom_bar(stat="identity")+coord_flip()+theme_classic()+
  xlab("Importance")+ylab("Feature")+ggtitle("Top 20 Important Features")+
  theme(legend.position="none")+scale_fill_continuous()
```
```{r,echo=FALSE,fig.width=7,fig.height=7}
print(plot)
```

**Model Performance**

Computing the model performance by predicting the classe from the training set, testing partition. The confusion matrix indicates the model predicted the wrong class in only a handful of cases.

```{r,echo=TRUE}
pred<-predict(mrf,train.test)
confMat<-confusionMatrix(pred,train.test$classe)

dplot<-data.frame("pred"=rep(c("A","B","C","D","E"),each=5),
                  "ref"=rep(c("A","B","C","D","E"),5),
                  "freq"=c(confMat$table[1,]/sum(confMat$table[1,]),
                           confMat$table[2,]/sum(confMat$table[2,]),
                           confMat$table[3,]/sum(confMat$table[3,]),
                           confMat$table[4,]/sum(confMat$table[4,]),
                           confMat$table[5,]/sum(confMat$table[5,])))

dplot$freq<-round(dplot$freq,4)

get_col <- colorRamp(c("dodgerblue4","deepskyblue1"))
quantiles <- (0:4) / 4
quantile.vals <- c(quantile(dplot$freq, quantiles, names=F)[2:5],1)
colours <- rgb(get_col(quantiles), max=255)
val.remap <- (quantile.vals - min(dplot$freq))

plot <- ggplot(dplot, aes(x=pred, y=ref, fill=freq))+ geom_tile()+theme_classic()+
  xlab("Predicted Class")+ylab("Actual Class")+labs(fill="Normalized\nFrequency")+ggtitle("Confusion Matrix")+
  scale_fill_gradientn(colours=colours,values=val.remap,breaks=quantile.vals,guide="legend")
```
```{r,echo=FALSE,fig.width=7,fig.height=7}
print(plot)
```

What is the accuracy and estimated out-of-sample error?
```{r,echo=TRUE}
acc<-confMat$overall[1]
oose<-1-acc

acc
oose
```

The model is therefore *99.2% accurate* and the *out-of-sample error rate is 0.76%*.

**Predicting the test set**
```{r,echo=TRUE}
result<-predict(mrf, test2)
result
```